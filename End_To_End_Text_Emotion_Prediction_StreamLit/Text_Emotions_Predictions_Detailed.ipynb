{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b9842be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d133642",
   "metadata": {},
   "source": [
    "# Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fbca6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customized function to sepearate label and text\n",
    "\n",
    "def read_data(file):\n",
    "    data = []\n",
    "    \n",
    "    with open(file,'r') as f:\n",
    "        for line in f:\n",
    "            #print(line)\n",
    "            line = line.strip()\n",
    "            print(\"Line: \",line)\n",
    "            \n",
    "            print(\"Label from [ ]: \",line[1:line.find(\"]\")])\n",
    "            print(\"Label stripped for starting and trailing spaces: \",line[1:line.find(\"]\")].strip())\n",
    "            print(\"Label split by spaces: \",line[1:line.find(\"]\")].split())\n",
    "            print(\"Label joined with a space to convert to string: \",' '.join(line[1:line.find(\"]\")].split()))\n",
    "            \n",
    "            label = ' '.join(line[1:line.find(\"]\")].strip().split())\n",
    "            print(\"Label Data: \",label)\n",
    "            print(\"Label datatype: \",type(label))\n",
    "            text = line[line.find(']')+1:].strip()\n",
    "            print(\"Text Data: \",text)\n",
    "            data.append([label,text])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "619d11aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line:  [ 1.  0.  0.  0.  0.  0.  0.] During the period of falling in love, each time that we met and especially when we had not met for a long time.\n",
      "Label from [ ]:   1.  0.  0.  0.  0.  0.  0.\n",
      "Label stripped for starting and trailing spaces:  1.  0.  0.  0.  0.  0.  0.\n",
      "Label split by spaces:  ['1.', '0.', '0.', '0.', '0.', '0.', '0.']\n",
      "Label joined with a space to convert to string:  1. 0. 0. 0. 0. 0. 0.\n",
      "Label Data:  1. 0. 0. 0. 0. 0. 0.\n",
      "Label datatype:  <class 'str'>\n",
      "Text Data:  During the period of falling in love, each time that we met and especially when we had not met for a long time.\n",
      "Line:  [ 0.  1.  0.  0.  0.  0.  0.] When I was involved in a traffic accident.\n",
      "Label from [ ]:   0.  1.  0.  0.  0.  0.  0.\n",
      "Label stripped for starting and trailing spaces:  0.  1.  0.  0.  0.  0.  0.\n",
      "Label split by spaces:  ['0.', '1.', '0.', '0.', '0.', '0.', '0.']\n",
      "Label joined with a space to convert to string:  0. 1. 0. 0. 0. 0. 0.\n",
      "Label Data:  0. 1. 0. 0. 0. 0. 0.\n",
      "Label datatype:  <class 'str'>\n",
      "Text Data:  When I was involved in a traffic accident.\n",
      "Line:  [ 0.  0.  1.  0.  0.  0.  0.] When I was driving home after  several days of hard work, there was a motorist ahead of me who was driving at 50 km/hour and refused, despite his low speeed to let me overtake.\n",
      "Label from [ ]:   0.  0.  1.  0.  0.  0.  0.\n",
      "Label stripped for starting and trailing spaces:  0.  0.  1.  0.  0.  0.  0.\n",
      "Label split by spaces:  ['0.', '0.', '1.', '0.', '0.', '0.', '0.']\n",
      "Label joined with a space to convert to string:  0. 0. 1. 0. 0. 0. 0.\n",
      "Label Data:  0. 0. 1. 0. 0. 0. 0.\n",
      "Label datatype:  <class 'str'>\n",
      "Text Data:  When I was driving home after  several days of hard work, there was a motorist ahead of me who was driving at 50 km/hour and refused, despite his low speeed to let me overtake.\n",
      "Line:  [ 0.  0.  0.  1.  0.  0.  0.] When I lost the person who meant the most to me.\n",
      "Label from [ ]:   0.  0.  0.  1.  0.  0.  0.\n",
      "Label stripped for starting and trailing spaces:  0.  0.  0.  1.  0.  0.  0.\n",
      "Label split by spaces:  ['0.', '0.', '0.', '1.', '0.', '0.', '0.']\n",
      "Label joined with a space to convert to string:  0. 0. 0. 1. 0. 0. 0.\n",
      "Label Data:  0. 0. 0. 1. 0. 0. 0.\n",
      "Label datatype:  <class 'str'>\n",
      "Text Data:  When I lost the person who meant the most to me.\n",
      "Line:  [ 0.  0.  0.  0.  1.  0.  0.] The time I knocked a deer down - the sight of the animal's injuries and helplessness.  The realization that the animal was so badly hurt that it had to be put down, and when the animal screamed at the moment of death.\n",
      "Label from [ ]:   0.  0.  0.  0.  1.  0.  0.\n",
      "Label stripped for starting and trailing spaces:  0.  0.  0.  0.  1.  0.  0.\n",
      "Label split by spaces:  ['0.', '0.', '0.', '0.', '1.', '0.', '0.']\n",
      "Label joined with a space to convert to string:  0. 0. 0. 0. 1. 0. 0.\n",
      "Label Data:  0. 0. 0. 0. 1. 0. 0.\n",
      "Label datatype:  <class 'str'>\n",
      "Text Data:  The time I knocked a deer down - the sight of the animal's injuries and helplessness.  The realization that the animal was so badly hurt that it had to be put down, and when the animal screamed at the moment of death.\n",
      "Number of instances: 5\n"
     ]
    }
   ],
   "source": [
    "file ='dummy_data.txt'       #'text_emot.txt'\n",
    "data = read_data(file)\n",
    "print(\"Number of instances: {}\".format(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e00acd",
   "metadata": {},
   "source": [
    "# Tokenization and Generating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41bb7d3",
   "metadata": {},
   "source": [
    "N-grams are continuous sequences of words or symbols, or tokens in a document. In technical terms, they can be defined as the neighboring sequences of items in a document\n",
    "\n",
    "So, [^a-z0-9#] will match any single character that is not a lowercase letter, digit, or hash symbol.\n",
    "\n",
    "Special characters are replaced with space and only lowercase a-z, numbers 0-9, hash symbol retained\n",
    "\n",
    "<b>For example:</b>\n",
    "\n",
    "Before:\n",
    "during the period of falling in love, each time that we met and especially when we had not met for a long time.\n",
    "\n",
    "After, re.sub[^a-z0-9#] = comma and full stop at the end is removed and space added\n",
    "\n",
    "during the period of falling in love  each time that we met and especially when we had not met for a long time\n",
    "\n",
    "\n",
    "text_punc = re.sub('[a-z0-9]',' ',text)\n",
    "\n",
    "In the above statement, where letters are lowercase/numbers are replaced with space and special characters such as ',' , '.' , '/' etc are retained/returned to text_punc\n",
    "\n",
    "<b>nrange</b>\n",
    "\n",
    "This variable which is tuple will suggest how many number of words to be grouped for creating features starting from 1 word to infinity\n",
    "\n",
    "For example:\n",
    "\n",
    "nrange=(1,1) # Means feature with one word to be created\n",
    "\n",
    "Text_Features = ['when', 'i', 'was', 'involved', 'in', 'a', 'traffic', 'accident']\n",
    "\n",
    "nrange = (1,2) # Here 1 word feature and 2 words features are created\n",
    "\n",
    "Text Features:  ['when', 'i', 'was', 'involved', 'in', 'a', 'traffic', 'accident', 'when i', 'i was', 'was involved', 'involved in', 'in a', 'a traffic', 'traffic accident']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9189dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customized function for tokenization and feature generation\n",
    "def ngram(token,n):\n",
    "    output = []\n",
    "    print(\"Token:\",token)\n",
    "    for i in range(n-1,len(token)):\n",
    "        print(i)\n",
    "        ngram = ' '.join(token[i-n+1:i+1])\n",
    "        print(\"Ngram Words: \",ngram)\n",
    "        output.append(ngram)\n",
    "    return output\n",
    "\n",
    "def create_feature(text, nrange=(1,1)):\n",
    "    text_features = []\n",
    "    text = text.lower()\n",
    "    print(\"Text after lowercase: \",text)\n",
    "    \n",
    "    text_alphanum = re.sub('[^a-z0-9#]',' ',text)\n",
    "    print(\"Text after retaining alphanumeric: \",text_alphanum)\n",
    "    \n",
    "    print(nrange[0],nrange[1])\n",
    "    for n in range(nrange[0],nrange[1]+1):\n",
    "        print(n)\n",
    "        \n",
    "        text_features += ngram(text_alphanum.split(),n)\n",
    "        print(\"Text Features: \",text_features)\n",
    "        \n",
    "    text_punc = re.sub('[a-z0-9]',' ',text)\n",
    "    print(\"Punctuation: \",text_punc)\n",
    "    \n",
    "    \n",
    "    text_features += ngram(text_punc.split(),1)\n",
    "    print(\"Text Features with punctuation: \",text_features)\n",
    "    \n",
    "    return Counter(text_features)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb1b6f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d4b9ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for _,text in data:\\n    create_feature(text)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for _,text in data:\n",
    "    create_feature(text)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8564362",
   "metadata": {},
   "source": [
    "# Store and convert labels to Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15f1930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize function to convert labels to emotions\n",
    "def convert_label(item,name):\n",
    "    print(\"Item: \",item)\n",
    "    print(\"Emotions: \",name)\n",
    "    \n",
    "    items = list(map(float, item.split()))\n",
    "    print(\"Labels converted to float list: \",items)\n",
    "    \n",
    "    label = \"\"\n",
    "    \n",
    "    for idx in range(len(items)):\n",
    "        print(idx)\n",
    "        \n",
    "        if items[idx] == 1:\n",
    "            label += name[idx] +\" \"\n",
    "    \n",
    "    print(\"Label with indicies: \",label)\n",
    "    return label.strip()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fabcae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item:  1. 0. 0. 0. 0. 0. 0.\n",
      "Emotions:  ['joy', 'fear', 'anger', 'sadness', 'disgust', 'shame', 'guilt']\n",
      "Labels converted to float list:  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Label with indicies:  joy \n",
      "Text after lowercase:  during the period of falling in love, each time that we met and especially when we had not met for a long time.\n",
      "Text after retaining alphanumeric:  during the period of falling in love  each time that we met and especially when we had not met for a long time \n",
      "1 2\n",
      "1\n",
      "Token: ['during', 'the', 'period', 'of', 'falling', 'in', 'love', 'each', 'time', 'that', 'we', 'met', 'and', 'especially', 'when', 'we', 'had', 'not', 'met', 'for', 'a', 'long', 'time']\n",
      "0\n",
      "Ngram Words:  during\n",
      "1\n",
      "Ngram Words:  the\n",
      "2\n",
      "Ngram Words:  period\n",
      "3\n",
      "Ngram Words:  of\n",
      "4\n",
      "Ngram Words:  falling\n",
      "5\n",
      "Ngram Words:  in\n",
      "6\n",
      "Ngram Words:  love\n",
      "7\n",
      "Ngram Words:  each\n",
      "8\n",
      "Ngram Words:  time\n",
      "9\n",
      "Ngram Words:  that\n",
      "10\n",
      "Ngram Words:  we\n",
      "11\n",
      "Ngram Words:  met\n",
      "12\n",
      "Ngram Words:  and\n",
      "13\n",
      "Ngram Words:  especially\n",
      "14\n",
      "Ngram Words:  when\n",
      "15\n",
      "Ngram Words:  we\n",
      "16\n",
      "Ngram Words:  had\n",
      "17\n",
      "Ngram Words:  not\n",
      "18\n",
      "Ngram Words:  met\n",
      "19\n",
      "Ngram Words:  for\n",
      "20\n",
      "Ngram Words:  a\n",
      "21\n",
      "Ngram Words:  long\n",
      "22\n",
      "Ngram Words:  time\n",
      "Text Features:  ['during', 'the', 'period', 'of', 'falling', 'in', 'love', 'each', 'time', 'that', 'we', 'met', 'and', 'especially', 'when', 'we', 'had', 'not', 'met', 'for', 'a', 'long', 'time']\n",
      "2\n",
      "Token: ['during', 'the', 'period', 'of', 'falling', 'in', 'love', 'each', 'time', 'that', 'we', 'met', 'and', 'especially', 'when', 'we', 'had', 'not', 'met', 'for', 'a', 'long', 'time']\n",
      "1\n",
      "Ngram Words:  during the\n",
      "2\n",
      "Ngram Words:  the period\n",
      "3\n",
      "Ngram Words:  period of\n",
      "4\n",
      "Ngram Words:  of falling\n",
      "5\n",
      "Ngram Words:  falling in\n",
      "6\n",
      "Ngram Words:  in love\n",
      "7\n",
      "Ngram Words:  love each\n",
      "8\n",
      "Ngram Words:  each time\n",
      "9\n",
      "Ngram Words:  time that\n",
      "10\n",
      "Ngram Words:  that we\n",
      "11\n",
      "Ngram Words:  we met\n",
      "12\n",
      "Ngram Words:  met and\n",
      "13\n",
      "Ngram Words:  and especially\n",
      "14\n",
      "Ngram Words:  especially when\n",
      "15\n",
      "Ngram Words:  when we\n",
      "16\n",
      "Ngram Words:  we had\n",
      "17\n",
      "Ngram Words:  had not\n",
      "18\n",
      "Ngram Words:  not met\n",
      "19\n",
      "Ngram Words:  met for\n",
      "20\n",
      "Ngram Words:  for a\n",
      "21\n",
      "Ngram Words:  a long\n",
      "22\n",
      "Ngram Words:  long time\n",
      "Text Features:  ['during', 'the', 'period', 'of', 'falling', 'in', 'love', 'each', 'time', 'that', 'we', 'met', 'and', 'especially', 'when', 'we', 'had', 'not', 'met', 'for', 'a', 'long', 'time', 'during the', 'the period', 'period of', 'of falling', 'falling in', 'in love', 'love each', 'each time', 'time that', 'that we', 'we met', 'met and', 'and especially', 'especially when', 'when we', 'we had', 'had not', 'not met', 'met for', 'for a', 'a long', 'long time']\n",
      "Punctuation:                                      ,                                                                         .\n",
      "Token: [',', '.']\n",
      "0\n",
      "Ngram Words:  ,\n",
      "1\n",
      "Ngram Words:  .\n",
      "Text Features with punctuation:  ['during', 'the', 'period', 'of', 'falling', 'in', 'love', 'each', 'time', 'that', 'we', 'met', 'and', 'especially', 'when', 'we', 'had', 'not', 'met', 'for', 'a', 'long', 'time', 'during the', 'the period', 'period of', 'of falling', 'falling in', 'in love', 'love each', 'each time', 'time that', 'that we', 'we met', 'met and', 'and especially', 'especially when', 'when we', 'we had', 'had not', 'not met', 'met for', 'for a', 'a long', 'long time', ',', '.']\n",
      "Item:  0. 1. 0. 0. 0. 0. 0.\n",
      "Emotions:  ['joy', 'fear', 'anger', 'sadness', 'disgust', 'shame', 'guilt']\n",
      "Labels converted to float list:  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Label with indicies:  fear \n",
      "Text after lowercase:  when i was involved in a traffic accident.\n",
      "Text after retaining alphanumeric:  when i was involved in a traffic accident \n",
      "1 2\n",
      "1\n",
      "Token: ['when', 'i', 'was', 'involved', 'in', 'a', 'traffic', 'accident']\n",
      "0\n",
      "Ngram Words:  when\n",
      "1\n",
      "Ngram Words:  i\n",
      "2\n",
      "Ngram Words:  was\n",
      "3\n",
      "Ngram Words:  involved\n",
      "4\n",
      "Ngram Words:  in\n",
      "5\n",
      "Ngram Words:  a\n",
      "6\n",
      "Ngram Words:  traffic\n",
      "7\n",
      "Ngram Words:  accident\n",
      "Text Features:  ['when', 'i', 'was', 'involved', 'in', 'a', 'traffic', 'accident']\n",
      "2\n",
      "Token: ['when', 'i', 'was', 'involved', 'in', 'a', 'traffic', 'accident']\n",
      "1\n",
      "Ngram Words:  when i\n",
      "2\n",
      "Ngram Words:  i was\n",
      "3\n",
      "Ngram Words:  was involved\n",
      "4\n",
      "Ngram Words:  involved in\n",
      "5\n",
      "Ngram Words:  in a\n",
      "6\n",
      "Ngram Words:  a traffic\n",
      "7\n",
      "Ngram Words:  traffic accident\n",
      "Text Features:  ['when', 'i', 'was', 'involved', 'in', 'a', 'traffic', 'accident', 'when i', 'i was', 'was involved', 'involved in', 'in a', 'a traffic', 'traffic accident']\n",
      "Punctuation:                                           .\n",
      "Token: ['.']\n",
      "0\n",
      "Ngram Words:  .\n",
      "Text Features with punctuation:  ['when', 'i', 'was', 'involved', 'in', 'a', 'traffic', 'accident', 'when i', 'i was', 'was involved', 'involved in', 'in a', 'a traffic', 'traffic accident', '.']\n",
      "Item:  0. 0. 1. 0. 0. 0. 0.\n",
      "Emotions:  ['joy', 'fear', 'anger', 'sadness', 'disgust', 'shame', 'guilt']\n",
      "Labels converted to float list:  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Label with indicies:  anger \n",
      "Text after lowercase:  when i was driving home after  several days of hard work, there was a motorist ahead of me who was driving at 50 km/hour and refused, despite his low speeed to let me overtake.\n",
      "Text after retaining alphanumeric:  when i was driving home after  several days of hard work  there was a motorist ahead of me who was driving at 50 km hour and refused  despite his low speeed to let me overtake \n",
      "1 2\n",
      "1\n",
      "Token: ['when', 'i', 'was', 'driving', 'home', 'after', 'several', 'days', 'of', 'hard', 'work', 'there', 'was', 'a', 'motorist', 'ahead', 'of', 'me', 'who', 'was', 'driving', 'at', '50', 'km', 'hour', 'and', 'refused', 'despite', 'his', 'low', 'speeed', 'to', 'let', 'me', 'overtake']\n",
      "0\n",
      "Ngram Words:  when\n",
      "1\n",
      "Ngram Words:  i\n",
      "2\n",
      "Ngram Words:  was\n",
      "3\n",
      "Ngram Words:  driving\n",
      "4\n",
      "Ngram Words:  home\n",
      "5\n",
      "Ngram Words:  after\n",
      "6\n",
      "Ngram Words:  several\n",
      "7\n",
      "Ngram Words:  days\n",
      "8\n",
      "Ngram Words:  of\n",
      "9\n",
      "Ngram Words:  hard\n",
      "10\n",
      "Ngram Words:  work\n",
      "11\n",
      "Ngram Words:  there\n",
      "12\n",
      "Ngram Words:  was\n",
      "13\n",
      "Ngram Words:  a\n",
      "14\n",
      "Ngram Words:  motorist\n",
      "15\n",
      "Ngram Words:  ahead\n",
      "16\n",
      "Ngram Words:  of\n",
      "17\n",
      "Ngram Words:  me\n",
      "18\n",
      "Ngram Words:  who\n",
      "19\n",
      "Ngram Words:  was\n",
      "20\n",
      "Ngram Words:  driving\n",
      "21\n",
      "Ngram Words:  at\n",
      "22\n",
      "Ngram Words:  50\n",
      "23\n",
      "Ngram Words:  km\n",
      "24\n",
      "Ngram Words:  hour\n",
      "25\n",
      "Ngram Words:  and\n",
      "26\n",
      "Ngram Words:  refused\n",
      "27\n",
      "Ngram Words:  despite\n",
      "28\n",
      "Ngram Words:  his\n",
      "29\n",
      "Ngram Words:  low\n",
      "30\n",
      "Ngram Words:  speeed\n",
      "31\n",
      "Ngram Words:  to\n",
      "32\n",
      "Ngram Words:  let\n",
      "33\n",
      "Ngram Words:  me\n",
      "34\n",
      "Ngram Words:  overtake\n",
      "Text Features:  ['when', 'i', 'was', 'driving', 'home', 'after', 'several', 'days', 'of', 'hard', 'work', 'there', 'was', 'a', 'motorist', 'ahead', 'of', 'me', 'who', 'was', 'driving', 'at', '50', 'km', 'hour', 'and', 'refused', 'despite', 'his', 'low', 'speeed', 'to', 'let', 'me', 'overtake']\n",
      "2\n",
      "Token: ['when', 'i', 'was', 'driving', 'home', 'after', 'several', 'days', 'of', 'hard', 'work', 'there', 'was', 'a', 'motorist', 'ahead', 'of', 'me', 'who', 'was', 'driving', 'at', '50', 'km', 'hour', 'and', 'refused', 'despite', 'his', 'low', 'speeed', 'to', 'let', 'me', 'overtake']\n",
      "1\n",
      "Ngram Words:  when i\n",
      "2\n",
      "Ngram Words:  i was\n",
      "3\n",
      "Ngram Words:  was driving\n",
      "4\n",
      "Ngram Words:  driving home\n",
      "5\n",
      "Ngram Words:  home after\n",
      "6\n",
      "Ngram Words:  after several\n",
      "7\n",
      "Ngram Words:  several days\n",
      "8\n",
      "Ngram Words:  days of\n",
      "9\n",
      "Ngram Words:  of hard\n",
      "10\n",
      "Ngram Words:  hard work\n",
      "11\n",
      "Ngram Words:  work there\n",
      "12\n",
      "Ngram Words:  there was\n",
      "13\n",
      "Ngram Words:  was a\n",
      "14\n",
      "Ngram Words:  a motorist\n",
      "15\n",
      "Ngram Words:  motorist ahead\n",
      "16\n",
      "Ngram Words:  ahead of\n",
      "17\n",
      "Ngram Words:  of me\n",
      "18\n",
      "Ngram Words:  me who\n",
      "19\n",
      "Ngram Words:  who was\n",
      "20\n",
      "Ngram Words:  was driving\n",
      "21\n",
      "Ngram Words:  driving at\n",
      "22\n",
      "Ngram Words:  at 50\n",
      "23\n",
      "Ngram Words:  50 km\n",
      "24\n",
      "Ngram Words:  km hour\n",
      "25\n",
      "Ngram Words:  hour and\n",
      "26\n",
      "Ngram Words:  and refused\n",
      "27\n",
      "Ngram Words:  refused despite\n",
      "28\n",
      "Ngram Words:  despite his\n",
      "29\n",
      "Ngram Words:  his low\n",
      "30\n",
      "Ngram Words:  low speeed\n",
      "31\n",
      "Ngram Words:  speeed to\n",
      "32\n",
      "Ngram Words:  to let\n",
      "33\n",
      "Ngram Words:  let me\n",
      "34\n",
      "Ngram Words:  me overtake\n",
      "Text Features:  ['when', 'i', 'was', 'driving', 'home', 'after', 'several', 'days', 'of', 'hard', 'work', 'there', 'was', 'a', 'motorist', 'ahead', 'of', 'me', 'who', 'was', 'driving', 'at', '50', 'km', 'hour', 'and', 'refused', 'despite', 'his', 'low', 'speeed', 'to', 'let', 'me', 'overtake', 'when i', 'i was', 'was driving', 'driving home', 'home after', 'after several', 'several days', 'days of', 'of hard', 'hard work', 'work there', 'there was', 'was a', 'a motorist', 'motorist ahead', 'ahead of', 'of me', 'me who', 'who was', 'was driving', 'driving at', 'at 50', '50 km', 'km hour', 'hour and', 'and refused', 'refused despite', 'despite his', 'his low', 'low speeed', 'speeed to', 'to let', 'let me', 'me overtake']\n",
      "Punctuation:                                                          ,                                                          /                ,                                          .\n",
      "Token: [',', '/', ',', '.']\n",
      "0\n",
      "Ngram Words:  ,\n",
      "1\n",
      "Ngram Words:  /\n",
      "2\n",
      "Ngram Words:  ,\n",
      "3\n",
      "Ngram Words:  .\n",
      "Text Features with punctuation:  ['when', 'i', 'was', 'driving', 'home', 'after', 'several', 'days', 'of', 'hard', 'work', 'there', 'was', 'a', 'motorist', 'ahead', 'of', 'me', 'who', 'was', 'driving', 'at', '50', 'km', 'hour', 'and', 'refused', 'despite', 'his', 'low', 'speeed', 'to', 'let', 'me', 'overtake', 'when i', 'i was', 'was driving', 'driving home', 'home after', 'after several', 'several days', 'days of', 'of hard', 'hard work', 'work there', 'there was', 'was a', 'a motorist', 'motorist ahead', 'ahead of', 'of me', 'me who', 'who was', 'was driving', 'driving at', 'at 50', '50 km', 'km hour', 'hour and', 'and refused', 'refused despite', 'despite his', 'his low', 'low speeed', 'speeed to', 'to let', 'let me', 'me overtake', ',', '/', ',', '.']\n",
      "Item:  0. 0. 0. 1. 0. 0. 0.\n",
      "Emotions:  ['joy', 'fear', 'anger', 'sadness', 'disgust', 'shame', 'guilt']\n",
      "Labels converted to float list:  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Label with indicies:  sadness \n",
      "Text after lowercase:  when i lost the person who meant the most to me.\n",
      "Text after retaining alphanumeric:  when i lost the person who meant the most to me \n",
      "1 2\n",
      "1\n",
      "Token: ['when', 'i', 'lost', 'the', 'person', 'who', 'meant', 'the', 'most', 'to', 'me']\n",
      "0\n",
      "Ngram Words:  when\n",
      "1\n",
      "Ngram Words:  i\n",
      "2\n",
      "Ngram Words:  lost\n",
      "3\n",
      "Ngram Words:  the\n",
      "4\n",
      "Ngram Words:  person\n",
      "5\n",
      "Ngram Words:  who\n",
      "6\n",
      "Ngram Words:  meant\n",
      "7\n",
      "Ngram Words:  the\n",
      "8\n",
      "Ngram Words:  most\n",
      "9\n",
      "Ngram Words:  to\n",
      "10\n",
      "Ngram Words:  me\n",
      "Text Features:  ['when', 'i', 'lost', 'the', 'person', 'who', 'meant', 'the', 'most', 'to', 'me']\n",
      "2\n",
      "Token: ['when', 'i', 'lost', 'the', 'person', 'who', 'meant', 'the', 'most', 'to', 'me']\n",
      "1\n",
      "Ngram Words:  when i\n",
      "2\n",
      "Ngram Words:  i lost\n",
      "3\n",
      "Ngram Words:  lost the\n",
      "4\n",
      "Ngram Words:  the person\n",
      "5\n",
      "Ngram Words:  person who\n",
      "6\n",
      "Ngram Words:  who meant\n",
      "7\n",
      "Ngram Words:  meant the\n",
      "8\n",
      "Ngram Words:  the most\n",
      "9\n",
      "Ngram Words:  most to\n",
      "10\n",
      "Ngram Words:  to me\n",
      "Text Features:  ['when', 'i', 'lost', 'the', 'person', 'who', 'meant', 'the', 'most', 'to', 'me', 'when i', 'i lost', 'lost the', 'the person', 'person who', 'who meant', 'meant the', 'the most', 'most to', 'to me']\n",
      "Punctuation:                                                 .\n",
      "Token: ['.']\n",
      "0\n",
      "Ngram Words:  .\n",
      "Text Features with punctuation:  ['when', 'i', 'lost', 'the', 'person', 'who', 'meant', 'the', 'most', 'to', 'me', 'when i', 'i lost', 'lost the', 'the person', 'person who', 'who meant', 'meant the', 'the most', 'most to', 'to me', '.']\n",
      "Item:  0. 0. 0. 0. 1. 0. 0.\n",
      "Emotions:  ['joy', 'fear', 'anger', 'sadness', 'disgust', 'shame', 'guilt']\n",
      "Labels converted to float list:  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Label with indicies:  disgust \n",
      "Text after lowercase:  the time i knocked a deer down - the sight of the animal's injuries and helplessness.  the realization that the animal was so badly hurt that it had to be put down, and when the animal screamed at the moment of death.\n",
      "Text after retaining alphanumeric:  the time i knocked a deer down   the sight of the animal s injuries and helplessness   the realization that the animal was so badly hurt that it had to be put down  and when the animal screamed at the moment of death \n",
      "1 2\n",
      "1\n",
      "Token: ['the', 'time', 'i', 'knocked', 'a', 'deer', 'down', 'the', 'sight', 'of', 'the', 'animal', 's', 'injuries', 'and', 'helplessness', 'the', 'realization', 'that', 'the', 'animal', 'was', 'so', 'badly', 'hurt', 'that', 'it', 'had', 'to', 'be', 'put', 'down', 'and', 'when', 'the', 'animal', 'screamed', 'at', 'the', 'moment', 'of', 'death']\n",
      "0\n",
      "Ngram Words:  the\n",
      "1\n",
      "Ngram Words:  time\n",
      "2\n",
      "Ngram Words:  i\n",
      "3\n",
      "Ngram Words:  knocked\n",
      "4\n",
      "Ngram Words:  a\n",
      "5\n",
      "Ngram Words:  deer\n",
      "6\n",
      "Ngram Words:  down\n",
      "7\n",
      "Ngram Words:  the\n",
      "8\n",
      "Ngram Words:  sight\n",
      "9\n",
      "Ngram Words:  of\n",
      "10\n",
      "Ngram Words:  the\n",
      "11\n",
      "Ngram Words:  animal\n",
      "12\n",
      "Ngram Words:  s\n",
      "13\n",
      "Ngram Words:  injuries\n",
      "14\n",
      "Ngram Words:  and\n",
      "15\n",
      "Ngram Words:  helplessness\n",
      "16\n",
      "Ngram Words:  the\n",
      "17\n",
      "Ngram Words:  realization\n",
      "18\n",
      "Ngram Words:  that\n",
      "19\n",
      "Ngram Words:  the\n",
      "20\n",
      "Ngram Words:  animal\n",
      "21\n",
      "Ngram Words:  was\n",
      "22\n",
      "Ngram Words:  so\n",
      "23\n",
      "Ngram Words:  badly\n",
      "24\n",
      "Ngram Words:  hurt\n",
      "25\n",
      "Ngram Words:  that\n",
      "26\n",
      "Ngram Words:  it\n",
      "27\n",
      "Ngram Words:  had\n",
      "28\n",
      "Ngram Words:  to\n",
      "29\n",
      "Ngram Words:  be\n",
      "30\n",
      "Ngram Words:  put\n",
      "31\n",
      "Ngram Words:  down\n",
      "32\n",
      "Ngram Words:  and\n",
      "33\n",
      "Ngram Words:  when\n",
      "34\n",
      "Ngram Words:  the\n",
      "35\n",
      "Ngram Words:  animal\n",
      "36\n",
      "Ngram Words:  screamed\n",
      "37\n",
      "Ngram Words:  at\n",
      "38\n",
      "Ngram Words:  the\n",
      "39\n",
      "Ngram Words:  moment\n",
      "40\n",
      "Ngram Words:  of\n",
      "41\n",
      "Ngram Words:  death\n",
      "Text Features:  ['the', 'time', 'i', 'knocked', 'a', 'deer', 'down', 'the', 'sight', 'of', 'the', 'animal', 's', 'injuries', 'and', 'helplessness', 'the', 'realization', 'that', 'the', 'animal', 'was', 'so', 'badly', 'hurt', 'that', 'it', 'had', 'to', 'be', 'put', 'down', 'and', 'when', 'the', 'animal', 'screamed', 'at', 'the', 'moment', 'of', 'death']\n",
      "2\n",
      "Token: ['the', 'time', 'i', 'knocked', 'a', 'deer', 'down', 'the', 'sight', 'of', 'the', 'animal', 's', 'injuries', 'and', 'helplessness', 'the', 'realization', 'that', 'the', 'animal', 'was', 'so', 'badly', 'hurt', 'that', 'it', 'had', 'to', 'be', 'put', 'down', 'and', 'when', 'the', 'animal', 'screamed', 'at', 'the', 'moment', 'of', 'death']\n",
      "1\n",
      "Ngram Words:  the time\n",
      "2\n",
      "Ngram Words:  time i\n",
      "3\n",
      "Ngram Words:  i knocked\n",
      "4\n",
      "Ngram Words:  knocked a\n",
      "5\n",
      "Ngram Words:  a deer\n",
      "6\n",
      "Ngram Words:  deer down\n",
      "7\n",
      "Ngram Words:  down the\n",
      "8\n",
      "Ngram Words:  the sight\n",
      "9\n",
      "Ngram Words:  sight of\n",
      "10\n",
      "Ngram Words:  of the\n",
      "11\n",
      "Ngram Words:  the animal\n",
      "12\n",
      "Ngram Words:  animal s\n",
      "13\n",
      "Ngram Words:  s injuries\n",
      "14\n",
      "Ngram Words:  injuries and\n",
      "15\n",
      "Ngram Words:  and helplessness\n",
      "16\n",
      "Ngram Words:  helplessness the\n",
      "17\n",
      "Ngram Words:  the realization\n",
      "18\n",
      "Ngram Words:  realization that\n",
      "19\n",
      "Ngram Words:  that the\n",
      "20\n",
      "Ngram Words:  the animal\n",
      "21\n",
      "Ngram Words:  animal was\n",
      "22\n",
      "Ngram Words:  was so\n",
      "23\n",
      "Ngram Words:  so badly\n",
      "24\n",
      "Ngram Words:  badly hurt\n",
      "25\n",
      "Ngram Words:  hurt that\n",
      "26\n",
      "Ngram Words:  that it\n",
      "27\n",
      "Ngram Words:  it had\n",
      "28\n",
      "Ngram Words:  had to\n",
      "29\n",
      "Ngram Words:  to be\n",
      "30\n",
      "Ngram Words:  be put\n",
      "31\n",
      "Ngram Words:  put down\n",
      "32\n",
      "Ngram Words:  down and\n",
      "33\n",
      "Ngram Words:  and when\n",
      "34\n",
      "Ngram Words:  when the\n",
      "35\n",
      "Ngram Words:  the animal\n",
      "36\n",
      "Ngram Words:  animal screamed\n",
      "37\n",
      "Ngram Words:  screamed at\n",
      "38\n",
      "Ngram Words:  at the\n",
      "39\n",
      "Ngram Words:  the moment\n",
      "40\n",
      "Ngram Words:  moment of\n",
      "41\n",
      "Ngram Words:  of death\n",
      "Text Features:  ['the', 'time', 'i', 'knocked', 'a', 'deer', 'down', 'the', 'sight', 'of', 'the', 'animal', 's', 'injuries', 'and', 'helplessness', 'the', 'realization', 'that', 'the', 'animal', 'was', 'so', 'badly', 'hurt', 'that', 'it', 'had', 'to', 'be', 'put', 'down', 'and', 'when', 'the', 'animal', 'screamed', 'at', 'the', 'moment', 'of', 'death', 'the time', 'time i', 'i knocked', 'knocked a', 'a deer', 'deer down', 'down the', 'the sight', 'sight of', 'of the', 'the animal', 'animal s', 's injuries', 'injuries and', 'and helplessness', 'helplessness the', 'the realization', 'realization that', 'that the', 'the animal', 'animal was', 'was so', 'so badly', 'badly hurt', 'hurt that', 'that it', 'it had', 'had to', 'to be', 'be put', 'put down', 'down and', 'and when', 'when the', 'the animal', 'animal screamed', 'screamed at', 'at the', 'the moment', 'moment of', 'of death']\n",
      "Punctuation:                                 -                        '                           .                                                                              ,                                                    .\n",
      "Token: ['-', \"'\", '.', ',', '.']\n",
      "0\n",
      "Ngram Words:  -\n",
      "1\n",
      "Ngram Words:  '\n",
      "2\n",
      "Ngram Words:  .\n",
      "3\n",
      "Ngram Words:  ,\n",
      "4\n",
      "Ngram Words:  .\n",
      "Text Features with punctuation:  ['the', 'time', 'i', 'knocked', 'a', 'deer', 'down', 'the', 'sight', 'of', 'the', 'animal', 's', 'injuries', 'and', 'helplessness', 'the', 'realization', 'that', 'the', 'animal', 'was', 'so', 'badly', 'hurt', 'that', 'it', 'had', 'to', 'be', 'put', 'down', 'and', 'when', 'the', 'animal', 'screamed', 'at', 'the', 'moment', 'of', 'death', 'the time', 'time i', 'i knocked', 'knocked a', 'a deer', 'deer down', 'down the', 'the sight', 'sight of', 'of the', 'the animal', 'animal s', 's injuries', 'injuries and', 'and helplessness', 'helplessness the', 'the realization', 'realization that', 'that the', 'the animal', 'animal was', 'was so', 'so badly', 'badly hurt', 'hurt that', 'that it', 'it had', 'had to', 'to be', 'be put', 'put down', 'down and', 'and when', 'when the', 'the animal', 'animal screamed', 'screamed at', 'at the', 'the moment', 'moment of', 'of death', '-', \"'\", '.', ',', '.']\n"
     ]
    }
   ],
   "source": [
    "emotions = [\"joy\",\"fear\",\"anger\",\"sadness\", \"disgust\", \"shame\", \"guilt\"]\n",
    "\n",
    "X_all = []\n",
    "y_all = []\n",
    "\n",
    "for label,text in data:\n",
    "    #label = list(map(int,label.split()))\n",
    "    #print(type(label))\n",
    "    y_all.append(convert_label(label,emotions))\n",
    "    X_all.append(create_feature(text,nrange=(1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c84d560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'during': 1,\n",
       "          'the': 1,\n",
       "          'period': 1,\n",
       "          'of': 1,\n",
       "          'falling': 1,\n",
       "          'in': 1,\n",
       "          'love': 1,\n",
       "          'each': 1,\n",
       "          'time': 2,\n",
       "          'that': 1,\n",
       "          'we': 2,\n",
       "          'met': 2,\n",
       "          'and': 1,\n",
       "          'especially': 1,\n",
       "          'when': 1,\n",
       "          'had': 1,\n",
       "          'not': 1,\n",
       "          'for': 1,\n",
       "          'a': 1,\n",
       "          'long': 1,\n",
       "          'during the': 1,\n",
       "          'the period': 1,\n",
       "          'period of': 1,\n",
       "          'of falling': 1,\n",
       "          'falling in': 1,\n",
       "          'in love': 1,\n",
       "          'love each': 1,\n",
       "          'each time': 1,\n",
       "          'time that': 1,\n",
       "          'that we': 1,\n",
       "          'we met': 1,\n",
       "          'met and': 1,\n",
       "          'and especially': 1,\n",
       "          'especially when': 1,\n",
       "          'when we': 1,\n",
       "          'we had': 1,\n",
       "          'had not': 1,\n",
       "          'not met': 1,\n",
       "          'met for': 1,\n",
       "          'for a': 1,\n",
       "          'a long': 1,\n",
       "          'long time': 1,\n",
       "          ',': 1,\n",
       "          '.': 1}),\n",
       " Counter({'when': 1,\n",
       "          'i': 1,\n",
       "          'was': 1,\n",
       "          'involved': 1,\n",
       "          'in': 1,\n",
       "          'a': 1,\n",
       "          'traffic': 1,\n",
       "          'accident': 1,\n",
       "          'when i': 1,\n",
       "          'i was': 1,\n",
       "          'was involved': 1,\n",
       "          'involved in': 1,\n",
       "          'in a': 1,\n",
       "          'a traffic': 1,\n",
       "          'traffic accident': 1,\n",
       "          '.': 1}),\n",
       " Counter({'when': 1,\n",
       "          'i': 1,\n",
       "          'was': 3,\n",
       "          'driving': 2,\n",
       "          'home': 1,\n",
       "          'after': 1,\n",
       "          'several': 1,\n",
       "          'days': 1,\n",
       "          'of': 2,\n",
       "          'hard': 1,\n",
       "          'work': 1,\n",
       "          'there': 1,\n",
       "          'a': 1,\n",
       "          'motorist': 1,\n",
       "          'ahead': 1,\n",
       "          'me': 2,\n",
       "          'who': 1,\n",
       "          'at': 1,\n",
       "          '50': 1,\n",
       "          'km': 1,\n",
       "          'hour': 1,\n",
       "          'and': 1,\n",
       "          'refused': 1,\n",
       "          'despite': 1,\n",
       "          'his': 1,\n",
       "          'low': 1,\n",
       "          'speeed': 1,\n",
       "          'to': 1,\n",
       "          'let': 1,\n",
       "          'overtake': 1,\n",
       "          'when i': 1,\n",
       "          'i was': 1,\n",
       "          'was driving': 2,\n",
       "          'driving home': 1,\n",
       "          'home after': 1,\n",
       "          'after several': 1,\n",
       "          'several days': 1,\n",
       "          'days of': 1,\n",
       "          'of hard': 1,\n",
       "          'hard work': 1,\n",
       "          'work there': 1,\n",
       "          'there was': 1,\n",
       "          'was a': 1,\n",
       "          'a motorist': 1,\n",
       "          'motorist ahead': 1,\n",
       "          'ahead of': 1,\n",
       "          'of me': 1,\n",
       "          'me who': 1,\n",
       "          'who was': 1,\n",
       "          'driving at': 1,\n",
       "          'at 50': 1,\n",
       "          '50 km': 1,\n",
       "          'km hour': 1,\n",
       "          'hour and': 1,\n",
       "          'and refused': 1,\n",
       "          'refused despite': 1,\n",
       "          'despite his': 1,\n",
       "          'his low': 1,\n",
       "          'low speeed': 1,\n",
       "          'speeed to': 1,\n",
       "          'to let': 1,\n",
       "          'let me': 1,\n",
       "          'me overtake': 1,\n",
       "          ',': 2,\n",
       "          '/': 1,\n",
       "          '.': 1}),\n",
       " Counter({'when': 1,\n",
       "          'i': 1,\n",
       "          'lost': 1,\n",
       "          'the': 2,\n",
       "          'person': 1,\n",
       "          'who': 1,\n",
       "          'meant': 1,\n",
       "          'most': 1,\n",
       "          'to': 1,\n",
       "          'me': 1,\n",
       "          'when i': 1,\n",
       "          'i lost': 1,\n",
       "          'lost the': 1,\n",
       "          'the person': 1,\n",
       "          'person who': 1,\n",
       "          'who meant': 1,\n",
       "          'meant the': 1,\n",
       "          'the most': 1,\n",
       "          'most to': 1,\n",
       "          'to me': 1,\n",
       "          '.': 1}),\n",
       " Counter({'the': 7,\n",
       "          'time': 1,\n",
       "          'i': 1,\n",
       "          'knocked': 1,\n",
       "          'a': 1,\n",
       "          'deer': 1,\n",
       "          'down': 2,\n",
       "          'sight': 1,\n",
       "          'of': 2,\n",
       "          'animal': 3,\n",
       "          's': 1,\n",
       "          'injuries': 1,\n",
       "          'and': 2,\n",
       "          'helplessness': 1,\n",
       "          'realization': 1,\n",
       "          'that': 2,\n",
       "          'was': 1,\n",
       "          'so': 1,\n",
       "          'badly': 1,\n",
       "          'hurt': 1,\n",
       "          'it': 1,\n",
       "          'had': 1,\n",
       "          'to': 1,\n",
       "          'be': 1,\n",
       "          'put': 1,\n",
       "          'when': 1,\n",
       "          'screamed': 1,\n",
       "          'at': 1,\n",
       "          'moment': 1,\n",
       "          'death': 1,\n",
       "          'the time': 1,\n",
       "          'time i': 1,\n",
       "          'i knocked': 1,\n",
       "          'knocked a': 1,\n",
       "          'a deer': 1,\n",
       "          'deer down': 1,\n",
       "          'down the': 1,\n",
       "          'the sight': 1,\n",
       "          'sight of': 1,\n",
       "          'of the': 1,\n",
       "          'the animal': 3,\n",
       "          'animal s': 1,\n",
       "          's injuries': 1,\n",
       "          'injuries and': 1,\n",
       "          'and helplessness': 1,\n",
       "          'helplessness the': 1,\n",
       "          'the realization': 1,\n",
       "          'realization that': 1,\n",
       "          'that the': 1,\n",
       "          'animal was': 1,\n",
       "          'was so': 1,\n",
       "          'so badly': 1,\n",
       "          'badly hurt': 1,\n",
       "          'hurt that': 1,\n",
       "          'that it': 1,\n",
       "          'it had': 1,\n",
       "          'had to': 1,\n",
       "          'to be': 1,\n",
       "          'be put': 1,\n",
       "          'put down': 1,\n",
       "          'down and': 1,\n",
       "          'and when': 1,\n",
       "          'when the': 1,\n",
       "          'animal screamed': 1,\n",
       "          'screamed at': 1,\n",
       "          'at the': 1,\n",
       "          'the moment': 1,\n",
       "          'moment of': 1,\n",
       "          'of death': 1,\n",
       "          '-': 1,\n",
       "          \"'\": 1,\n",
       "          '.': 2,\n",
       "          ',': 1})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "489b3860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['joy', 'fear', 'anger', 'sadness', 'disgust']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c34c9",
   "metadata": {},
   "source": [
    "# Splitting Data into Training and Test data\n",
    "\n",
    "\n",
    "The class DictVectorizer can be used to convert feature arrays represented as lists of standard Python dict objects to the NumPy/SciPy representation used by scikit-learn estimators.\n",
    "\n",
    "While not particularly fast to process, Python’s dict has the advantages of being convenient to use, being sparse (absent features need not be stored) and storing feature names in addition to values.\n",
    "\n",
    "DictVectorizer implements what is called one-of-K or “one-hot” coding for categorical (aka nominal, discrete) features. Categorical features are “attribute-value” pairs where the value is restricted to a list of discrete of possibilities without ordering (e.g. topic identifiers, types of objects, tags, names…).\n",
    "\n",
    "In the following, “city” is a categorical attribute while “temperature” is a traditional numerical feature:\n",
    "\n",
    "measurements = [\n",
    "\n",
    "{'city': 'Dubai', 'temperature': 33.},\n",
    "\n",
    "{'city': 'London', 'temperature': 12.},\n",
    "\n",
    "{'city': 'San Francisco', 'temperature': 18.},\n",
    "\n",
    "]\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vec = DictVectorizer()\n",
    "\n",
    "vec.fit_transform(measurements).toarray()\n",
    "\n",
    "array([[ 1., 0., 0., 33.],\n",
    "\n",
    "[ 0., 1., 0., 12.],\n",
    "\n",
    "[ 0., 0., 1., 18.]])\n",
    "\n",
    "vec.get_feature_names()\n",
    "\n",
    "['city=Dubai', 'city=London', 'city=San Francisco', 'temperature']\n",
    "\n",
    "Three first places are cities: 100 Dubai, 010 London, 001 Frisco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3837312",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X_all,y_all,test_size=0.1,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0df83ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'when': 1,\n",
       "          'i': 1,\n",
       "          'lost': 1,\n",
       "          'the': 2,\n",
       "          'person': 1,\n",
       "          'who': 1,\n",
       "          'meant': 1,\n",
       "          'most': 1,\n",
       "          'to': 1,\n",
       "          'me': 1,\n",
       "          'when i': 1,\n",
       "          'i lost': 1,\n",
       "          'lost the': 1,\n",
       "          'the person': 1,\n",
       "          'person who': 1,\n",
       "          'who meant': 1,\n",
       "          'meant the': 1,\n",
       "          'the most': 1,\n",
       "          'most to': 1,\n",
       "          'to me': 1,\n",
       "          '.': 1}),\n",
       " Counter({'the': 7,\n",
       "          'time': 1,\n",
       "          'i': 1,\n",
       "          'knocked': 1,\n",
       "          'a': 1,\n",
       "          'deer': 1,\n",
       "          'down': 2,\n",
       "          'sight': 1,\n",
       "          'of': 2,\n",
       "          'animal': 3,\n",
       "          's': 1,\n",
       "          'injuries': 1,\n",
       "          'and': 2,\n",
       "          'helplessness': 1,\n",
       "          'realization': 1,\n",
       "          'that': 2,\n",
       "          'was': 1,\n",
       "          'so': 1,\n",
       "          'badly': 1,\n",
       "          'hurt': 1,\n",
       "          'it': 1,\n",
       "          'had': 1,\n",
       "          'to': 1,\n",
       "          'be': 1,\n",
       "          'put': 1,\n",
       "          'when': 1,\n",
       "          'screamed': 1,\n",
       "          'at': 1,\n",
       "          'moment': 1,\n",
       "          'death': 1,\n",
       "          'the time': 1,\n",
       "          'time i': 1,\n",
       "          'i knocked': 1,\n",
       "          'knocked a': 1,\n",
       "          'a deer': 1,\n",
       "          'deer down': 1,\n",
       "          'down the': 1,\n",
       "          'the sight': 1,\n",
       "          'sight of': 1,\n",
       "          'of the': 1,\n",
       "          'the animal': 3,\n",
       "          'animal s': 1,\n",
       "          's injuries': 1,\n",
       "          'injuries and': 1,\n",
       "          'and helplessness': 1,\n",
       "          'helplessness the': 1,\n",
       "          'the realization': 1,\n",
       "          'realization that': 1,\n",
       "          'that the': 1,\n",
       "          'animal was': 1,\n",
       "          'was so': 1,\n",
       "          'so badly': 1,\n",
       "          'badly hurt': 1,\n",
       "          'hurt that': 1,\n",
       "          'that it': 1,\n",
       "          'it had': 1,\n",
       "          'had to': 1,\n",
       "          'to be': 1,\n",
       "          'be put': 1,\n",
       "          'put down': 1,\n",
       "          'down and': 1,\n",
       "          'and when': 1,\n",
       "          'when the': 1,\n",
       "          'animal screamed': 1,\n",
       "          'screamed at': 1,\n",
       "          'at the': 1,\n",
       "          'the moment': 1,\n",
       "          'moment of': 1,\n",
       "          'of death': 1,\n",
       "          '-': 1,\n",
       "          \"'\": 1,\n",
       "          '.': 2,\n",
       "          ',': 1}),\n",
       " Counter({'during': 1,\n",
       "          'the': 1,\n",
       "          'period': 1,\n",
       "          'of': 1,\n",
       "          'falling': 1,\n",
       "          'in': 1,\n",
       "          'love': 1,\n",
       "          'each': 1,\n",
       "          'time': 2,\n",
       "          'that': 1,\n",
       "          'we': 2,\n",
       "          'met': 2,\n",
       "          'and': 1,\n",
       "          'especially': 1,\n",
       "          'when': 1,\n",
       "          'had': 1,\n",
       "          'not': 1,\n",
       "          'for': 1,\n",
       "          'a': 1,\n",
       "          'long': 1,\n",
       "          'during the': 1,\n",
       "          'the period': 1,\n",
       "          'period of': 1,\n",
       "          'of falling': 1,\n",
       "          'falling in': 1,\n",
       "          'in love': 1,\n",
       "          'love each': 1,\n",
       "          'each time': 1,\n",
       "          'time that': 1,\n",
       "          'that we': 1,\n",
       "          'we met': 1,\n",
       "          'met and': 1,\n",
       "          'and especially': 1,\n",
       "          'especially when': 1,\n",
       "          'when we': 1,\n",
       "          'we had': 1,\n",
       "          'had not': 1,\n",
       "          'not met': 1,\n",
       "          'met for': 1,\n",
       "          'for a': 1,\n",
       "          'a long': 1,\n",
       "          'long time': 1,\n",
       "          ',': 1,\n",
       "          '.': 1}),\n",
       " Counter({'when': 1,\n",
       "          'i': 1,\n",
       "          'was': 3,\n",
       "          'driving': 2,\n",
       "          'home': 1,\n",
       "          'after': 1,\n",
       "          'several': 1,\n",
       "          'days': 1,\n",
       "          'of': 2,\n",
       "          'hard': 1,\n",
       "          'work': 1,\n",
       "          'there': 1,\n",
       "          'a': 1,\n",
       "          'motorist': 1,\n",
       "          'ahead': 1,\n",
       "          'me': 2,\n",
       "          'who': 1,\n",
       "          'at': 1,\n",
       "          '50': 1,\n",
       "          'km': 1,\n",
       "          'hour': 1,\n",
       "          'and': 1,\n",
       "          'refused': 1,\n",
       "          'despite': 1,\n",
       "          'his': 1,\n",
       "          'low': 1,\n",
       "          'speeed': 1,\n",
       "          'to': 1,\n",
       "          'let': 1,\n",
       "          'overtake': 1,\n",
       "          'when i': 1,\n",
       "          'i was': 1,\n",
       "          'was driving': 2,\n",
       "          'driving home': 1,\n",
       "          'home after': 1,\n",
       "          'after several': 1,\n",
       "          'several days': 1,\n",
       "          'days of': 1,\n",
       "          'of hard': 1,\n",
       "          'hard work': 1,\n",
       "          'work there': 1,\n",
       "          'there was': 1,\n",
       "          'was a': 1,\n",
       "          'a motorist': 1,\n",
       "          'motorist ahead': 1,\n",
       "          'ahead of': 1,\n",
       "          'of me': 1,\n",
       "          'me who': 1,\n",
       "          'who was': 1,\n",
       "          'driving at': 1,\n",
       "          'at 50': 1,\n",
       "          '50 km': 1,\n",
       "          'km hour': 1,\n",
       "          'hour and': 1,\n",
       "          'and refused': 1,\n",
       "          'refused despite': 1,\n",
       "          'despite his': 1,\n",
       "          'his low': 1,\n",
       "          'low speeed': 1,\n",
       "          'speeed to': 1,\n",
       "          'to let': 1,\n",
       "          'let me': 1,\n",
       "          'me overtake': 1,\n",
       "          ',': 2,\n",
       "          '/': 1,\n",
       "          '.': 1})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b425eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({'when': 1,\n",
       "          'i': 1,\n",
       "          'was': 1,\n",
       "          'involved': 1,\n",
       "          'in': 1,\n",
       "          'a': 1,\n",
       "          'traffic': 1,\n",
       "          'accident': 1,\n",
       "          'when i': 1,\n",
       "          'i was': 1,\n",
       "          'was involved': 1,\n",
       "          'involved in': 1,\n",
       "          'in a': 1,\n",
       "          'a traffic': 1,\n",
       "          'traffic accident': 1,\n",
       "          '.': 1})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ff2040a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customized function to run multiple models and note accuracy\n",
    "def train_test(clf,X_train,X_test,y_train,y_test):\n",
    "    clf.fit(X_train,y_train)\n",
    "    train_acc = accuracy_score(y_train,clf.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test,clf.predict(X_test))\n",
    "    return train_acc,test_acc\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "vect = DictVectorizer(sparse=True)\n",
    "X_train = vect.fit_transform(X_train)\n",
    "X_test = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcc5b5cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 2., 0.,\n",
       "        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 2., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 2.,\n",
       "        0., 1., 0., 1., 3., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 1., 1., 1., 0., 0., 2., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
       "        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 2., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 2., 1., 1., 0., 7., 3.,\n",
       "        1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
       "        1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 2., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
       "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 2., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 2., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 2., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 0., 1., 1., 0., 0., 0., 2., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 2., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 2., 0., 0.,\n",
       "        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
       "        3., 1., 2., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb0c6a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be58360e",
   "metadata": {},
   "source": [
    "# Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afde6d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Classifier                | Training Accuracy | Test Accuracy |\n",
      "| ------------------------- | ----------------- | ------------- |\n",
      "| SVC                       |         1.0000000 |     0.0000000 |\n",
      "| LinearSVC                 |         1.0000000 |     0.0000000 |\n",
      "| RandomForestClassifier    |         1.0000000 |     0.0000000 |\n",
      "| DecisionTreeClassifier    |         1.0000000 |     0.0000000 |\n"
     ]
    }
   ],
   "source": [
    "svc = SVC()\n",
    "lsvc = LinearSVC(random_state=123)\n",
    "rforest = RandomForestClassifier(random_state=123)\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "clifs = [svc,lsvc,rforest,dtree]\n",
    "\n",
    "#train and test them\n",
    "print(\"| {:25} | {} | {} |\".format(\"Classifier\", \"Training Accuracy\", \"Test Accuracy\"))\n",
    "print(\"| {} | {} | {} |\".format(\"-\"*25, \"-\"*17, \"-\"*13))\n",
    "\n",
    "for clf in clifs:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    train_acc,test_acc=train_test(clf,X_train,X_test,y_train,y_test)\n",
    "    print(\"| {:25} | {:17.7f} | {:13.7f} |\".format(clf_name, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fb41ad",
   "metadata": {},
   "source": [
    "# Frequency of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09004627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy       (1. 0. 0. 0. 0. 0. 0.)  1\n",
      "fear      (0. 1. 0. 0. 0. 0. 0.)  1\n",
      "anger     (0. 0. 1. 0. 0. 0. 0.)  1\n",
      "sadness   (0. 0. 0. 1. 0. 0. 0.)  1\n",
      "disgust   (0. 0. 0. 0. 1. 0. 0.)  1\n"
     ]
    }
   ],
   "source": [
    "l = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]\n",
    "\n",
    "l.sort()\n",
    "label_freq = {}\n",
    "\n",
    "for label,_ in data:\n",
    "    label_freq[label] = label_freq.get(label,0) + 1\n",
    "\n",
    "#Rewritten function without print statements\n",
    "def convert_label(item,name):        \n",
    "    items = list(map(float, item.split()))    \n",
    "    \n",
    "    label = \"\"\n",
    "    \n",
    "    for idx in range(len(items)):        \n",
    "        \n",
    "        if items[idx] == 1:\n",
    "            label += name[idx] +\" \"\n",
    "    \n",
    "    return label.strip()\n",
    "    \n",
    "# print the labels and their counts in sorted order\n",
    "for l in sorted(label_freq,key=label_freq.get,reverse=True):\n",
    "    print(\"{:10}({})  {}\".format(convert_label(l, emotions), l, label_freq[l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d215c83",
   "metadata": {},
   "source": [
    "# Detecting Emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62c69fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeated function withput print statements\n",
    "def ngram(token,n):\n",
    "    output = []    \n",
    "    for i in range(n-1,len(token)):        \n",
    "        ngram = ' '.join(token[i-n+1:i+1])        \n",
    "        output.append(ngram)\n",
    "    return output\n",
    "\n",
    "def create_feature(text, nrange=(1,1)):\n",
    "    text_features = []\n",
    "    text = text.lower()   \n",
    "    \n",
    "    text_alphanum = re.sub('[^a-z0-9#]',' ',text)    \n",
    "    \n",
    "    for n in range(nrange[0],nrange[1]+1):                \n",
    "        text_features += ngram(text_alphanum.split(),n)        \n",
    "        \n",
    "    text_punc = re.sub('[a-z0-9]',' ',text)    \n",
    "    \n",
    "    text_features += ngram(text_punc.split(),1)    \n",
    "    \n",
    "    return Counter(text_features) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e5a413",
   "metadata": {},
   "source": [
    "# Giving wrong prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e3ddb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "lsvc = LinearSVC(random_state=123)\n",
    "rforest = RandomForestClassifier(random_state=123)\n",
    "dtree = DecisionTreeClassifier()\n",
    "\n",
    "clifs = [svc,lsvc,rforest,dtree]\n",
    "\n",
    "emoji_dict = {\"joy\":\"😂\", \"fear\":\"😱\", \"anger\":\"😠\", \"sadness\":\"😢\", \"disgust\":\"😒\", \"shame\":\"😳\", \"guilt\":\"😳\"}\n",
    "\n",
    "t1 = \"This looks so impressive\"\n",
    "t2 = \"I have a fear of dogs\"\n",
    "t3 = \"My dog died yesterday\"\n",
    "t4 = \"I don't love you anymore..!\"\n",
    "\n",
    "texts = [t1, t2, t3, t4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e624fd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier()\n",
      "This looks so impressive 😂\n",
      "DecisionTreeClassifier()\n",
      "I have a fear of dogs 😂\n",
      "DecisionTreeClassifier()\n",
      "My dog died yesterday 😂\n",
      "DecisionTreeClassifier()\n",
      "I don't love you anymore..! 😂\n"
     ]
    }
   ],
   "source": [
    "for text in texts: \n",
    "    features = create_feature(text, nrange=(1, 4))\n",
    "    features = vect.transform(features)\n",
    "    print(clf)\n",
    "    prediction = clf.predict(features)[0]\n",
    "    print( text,emoji_dict[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e40e46e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
